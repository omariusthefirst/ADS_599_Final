{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the problem at hand, the most appropriate strategy was to fit a number of classification models to classify observations into vaccinated or un-vaccinated.\n",
    "\n",
    "The following models were fit, tested, and tuned to produce the most accurate model possible based on F-1 score: neural network model, logistic regression, SVM, gradient boosting classifier, Gaussian Naive Bayes, KNN, Random Forest, and a boosted classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard data analytical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os, warnings, time, dmba\n",
    "import scikitplot as skplt \n",
    "\n",
    "#Data Mining Book Libraries\n",
    "from dmba import liftChart, gainsChart,regressionSummary, classificationSummary, exhaustive_search\n",
    "from dmba import backward_elimination, forward_selection, stepwise_selection, adjusted_r2_score, AIC_score, BIC_score\n",
    "from os.path import exists\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc, roc_auc_score, plot_confusion_matrix,confusion_matrix,r2_score\n",
    "#Classification \n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,  LinearRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, kneighbors_graph\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Used to save keystrokes when wanting to print something. Now we can just use\n",
    "# p(\"Hello\") instead of print(\"Hello\")\n",
    "p = print\n",
    "# import csv\n",
    "# import re\n",
    "\n",
    "# Change this value if you are not using o_desktop\n",
    "computer = 'o_desktop'\n",
    "#computer = 'other'\n",
    "if (computer == 'o_desktop'):\n",
    "    os.environ['NUMEXPR_MAX_THREADS'] = '24'\n",
    "else:\n",
    "    # default is 4 or 8\n",
    "    os.environ['NUMEXPR_MAX_THREADS'] = '8'\n",
    "\n",
    "# For future use:\n",
    "# import threading\n",
    "# import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "      <th>seasonal_vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...  rent_or_own   employment_status  \\\n",
       "0                    1.0  ...          Own  Not in Labor Force   \n",
       "1                    1.0  ...         Rent            Employed   \n",
       "2                    0.0  ...          Own            Employed   \n",
       "3                    0.0  ...         Rent  Not in Labor Force   \n",
       "4                    1.0  ...          Own            Employed   \n",
       "\n",
       "   hhs_geo_region                census_msa  household_adults  \\\n",
       "0        oxchjgsf                   Non-MSA               0.0   \n",
       "1        bhuqouqj  MSA, Not Principle  City               0.0   \n",
       "2        qufhixun  MSA, Not Principle  City               2.0   \n",
       "3        lrircsnp       MSA, Principle City               0.0   \n",
       "4        qufhixun  MSA, Not Principle  City               1.0   \n",
       "\n",
       "   household_children  employment_industry  employment_occupation  \\\n",
       "0                 0.0                  NaN                    NaN   \n",
       "1                 0.0             pxcmvdjn               xgwztkwe   \n",
       "2                 0.0             rucpziij               xtkaffoo   \n",
       "3                 0.0                  NaN                    NaN   \n",
       "4                 0.0             wxleyezf               emcorrxb   \n",
       "\n",
       "   h1n1_vaccine  seasonal_vaccine  \n",
       "0             0                 0  \n",
       "1             0                 1  \n",
       "2             0                 0  \n",
       "3             0                 1  \n",
       "4             0                 0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting directories and loading training set and training labels\n",
    "repo_directory = r'C:/ADS_599_Final/'\n",
    "data_folder_directory = r'C:/ADS_599_Final/Data_Folder/'\n",
    "df_features_file = 'C:/ADS_599_Final/Data_Folder/training_set_features.csv'\n",
    "df_labels_file = 'C:/ADS_599_Final/Data_Folder/training_set_labels.csv'\n",
    "df = pd.read_csv(df_features_file)\n",
    "df_labels = pd.read_csv(df_labels_file)\n",
    "\n",
    "# Combining training data with training labels for modeling\n",
    "df = df.join(df_labels.set_index('respondent_id'), on='respondent_id')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming df\n",
    "df_train = df\n",
    "\n",
    "# Categories\n",
    "df_train['h1n1_concern'] = df_train['h1n1_concern'].fillna(-1)\n",
    "df_train['h1n1_knowledge'] = df_train['h1n1_knowledge'].fillna(-1)\n",
    "df_train['behavioral_antiviral_meds'] = df_train['behavioral_antiviral_meds'].fillna(-1)\n",
    "df_train['behavioral_avoidance'] = df_train['behavioral_avoidance'].fillna(-1)\n",
    "df_train['behavioral_face_mask'] = df_train['behavioral_face_mask'].fillna(-1)\n",
    "df_train['behavioral_large_gatherings'] = df_train['behavioral_large_gatherings'].fillna(-1)\n",
    "df_train['behavioral_outside_home'] = df_train['behavioral_outside_home'].fillna(-1)\n",
    "df_train['behavioral_wash_hands'] = df_train['behavioral_wash_hands'].fillna(-1)          \n",
    "df_train['behavioral_touch_face'] = df_train['behavioral_touch_face'].fillna(-1)\n",
    "df_train['doctor_recc_h1n1'] = df_train['doctor_recc_h1n1'].fillna(-1)\n",
    "df_train['doctor_recc_seasonal'] = df_train['doctor_recc_seasonal'].fillna(-1)\n",
    "df_train['chronic_med_condition'] = df_train['chronic_med_condition'].fillna(-1)\n",
    "df_train['child_under_6_months'] = df_train['child_under_6_months'].fillna(-1)\n",
    "df_train['health_worker'] = df_train['health_worker'].fillna(-1)\n",
    "df_train['health_insurance'] = df_train['health_insurance'].fillna(-1)\n",
    "df_train['opinion_h1n1_vacc_effective'] = df_train['opinion_h1n1_vacc_effective'].fillna(-1)\n",
    "df_train['opinion_h1n1_sick_from_vacc'] = df_train['opinion_h1n1_sick_from_vacc'].fillna(-1)\n",
    "df_train['opinion_h1n1_risk'] = df_train['opinion_h1n1_risk'].fillna(-1)\n",
    "df_train['opinion_seas_vacc_effective'] = df_train['opinion_seas_vacc_effective'].fillna(-1)\n",
    "df_train['opinion_seas_risk'] = df_train['opinion_seas_risk'].fillna(-1)\n",
    "df_train['opinion_seas_sick_from_vacc'] = df_train['opinion_seas_sick_from_vacc'].fillna(-1)\n",
    "df_train['household_adults'] = df_train['household_adults'].fillna(-1)\n",
    "df_train['household_children'] = df_train['household_children'].fillna(-1)\n",
    "\n",
    "# Numbers\n",
    "df_train['age_group'] = df_train['age_group'].fillna(\"no_response\")\n",
    "df_train['education'] = df_train['education'].fillna(\"no_response\")\n",
    "df_train['race'] = df_train['race'].fillna(\"no_response\")\n",
    "df_train['income_poverty'] = df_train['income_poverty'].fillna(\"no_response\")\n",
    "df_train['marital_status'] = df_train['marital_status'].fillna(\"no_response\")\n",
    "df_train['rent_or_own'] = df_train['rent_or_own'].fillna(\"no_response\")\n",
    "df_train['employment_status'] = df_train['employment_status'].fillna(\"no_response\")\n",
    "df_train['employment_occupation'] = df_train['employment_occupation'].fillna(\"no_response\")\n",
    "df_train['employment_industry'] = df_train['employment_industry'].fillna(\"no_response\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoding the null counts per column are: \n",
      "respondent_id                  0\n",
      "h1n1_concern                   0\n",
      "h1n1_knowledge                 0\n",
      "behavioral_antiviral_meds      0\n",
      "behavioral_avoidance           0\n",
      "behavioral_face_mask           0\n",
      "behavioral_wash_hands          0\n",
      "behavioral_large_gatherings    0\n",
      "behavioral_outside_home        0\n",
      "behavioral_touch_face          0\n",
      "doctor_recc_h1n1               0\n",
      "doctor_recc_seasonal           0\n",
      "chronic_med_condition          0\n",
      "child_under_6_months           0\n",
      "health_worker                  0\n",
      "health_insurance               0\n",
      "opinion_h1n1_vacc_effective    0\n",
      "opinion_h1n1_risk              0\n",
      "opinion_h1n1_sick_from_vacc    0\n",
      "opinion_seas_vacc_effective    0\n",
      "opinion_seas_risk              0\n",
      "opinion_seas_sick_from_vacc    0\n",
      "age_group                      0\n",
      "education                      0\n",
      "race                           0\n",
      "sex                            0\n",
      "income_poverty                 0\n",
      "marital_status                 0\n",
      "rent_or_own                    0\n",
      "employment_status              0\n",
      "hhs_geo_region                 0\n",
      "census_msa                     0\n",
      "household_adults               0\n",
      "household_children             0\n",
      "employment_industry            0\n",
      "employment_occupation          0\n",
      "h1n1_vaccine                   0\n",
      "seasonal_vaccine               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "df_train_label = df_train\n",
    "    # Encode labels the below is equivalent to df_train['hhs_geo_region']= label_encoder.fit_transform(df_train['hhs_geo_region'])\n",
    "df_train_label[\"hhs_geo_region\"] = df_train[\"hhs_geo_region\"].astype('category')\n",
    "df_train_label[\"hhs_geo_region\"] = df_train[\"hhs_geo_region\"].cat.codes\n",
    "df_train_label[\"census_msa\"] = df_train[\"census_msa\"].astype('category')\n",
    "df_train_label[\"census_msa\"] = df_train[\"census_msa\"].cat.codes\n",
    "df_train_label[\"employment_industry\"] = df_train[\"employment_industry\"].astype('category')\n",
    "df_train_label[\"employment_industry\"] = df_train[\"employment_industry\"].cat.codes\n",
    "df_train_label[\"employment_occupation\"] = df_train[\"employment_occupation\"].astype('category')\n",
    "df_train_label[\"employment_occupation\"] = df_train[\"employment_occupation\"].cat.codes\n",
    "df_train_label[\"employment_status\"] = df_train[\"employment_status\"].astype('category')\n",
    "df_train_label[\"employment_status\"] = df_train[\"employment_status\"].cat.codes\n",
    "df_train_label[\"rent_or_own\"] = df_train[\"rent_or_own\"].astype('category')\n",
    "df_train_label[\"rent_or_own\"] = df_train[\"rent_or_own\"].cat.codes\n",
    "df_train_label[\"marital_status\"] = df_train[\"marital_status\"].astype('category')\n",
    "df_train_label[\"marital_status\"] = df_train[\"marital_status\"].cat.codes\n",
    "df_train_label[\"income_poverty\"] = df_train[\"income_poverty\"].astype('category')\n",
    "df_train_label[\"income_poverty\"] = df_train[\"income_poverty\"].cat.codes\n",
    "df_train_label[\"race\"] = df_train[\"race\"].astype('category')\n",
    "df_train_label[\"race\"] = df_train[\"race\"].cat.codes\n",
    "df_train_label[\"education\"] = df_train[\"education\"].astype('category')\n",
    "df_train_label[\"education\"] = df_train[\"education\"].cat.codes\n",
    "df_train_label[\"age_group\"] = df_train[\"age_group\"].astype('category')\n",
    "df_train_label[\"age_group\"] = df_train[\"age_group\"].cat.codes\n",
    "df_train_label[\"sex\"] = df_train[\"sex\"].astype('category')\n",
    "df_train_label[\"sex\"] = df_train[\"sex\"].cat.codes\n",
    "\n",
    "p(\"After encoding the null counts per column are: \")\n",
    "p(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling nulls three ways\n",
    "\n",
    "handling_nulls = \"median\" # options \"median\" \"iterative\" \"dropall\"\n",
    "if handling_nulls == \"iterative\":\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train.replace(-1, np.nan) \n",
    "    df_train.replace(\"no_response\", np.nan) \n",
    "    \n",
    "    # SMOTE Sampling\n",
    "    temp_columns = df_train.columns\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    imp.fit(df_train)\n",
    "    df_train = pd.DataFrame(data=imp.transform(df_train))\n",
    "    df_train.columns = temp_columns\n",
    "    df_train\n",
    "elif handling_nulls == \"median\":\n",
    "    df_train_median = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_median.replace(-1, np.nan) \n",
    "    df_train_median.replace(\"no_response\", np.nan) \n",
    "    df_train_median.fillna(df_train.median())\n",
    "elif handling_nulls == \"dropall\":\n",
    "    df_train_drop = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_drop.replace(-1, np.nan) \n",
    "    df_train_drop.replace(\"no_response\", np.nan) \n",
    "    # See how it is if we drop the NaNs\n",
    "    df_train_drop = df_train.dropna(inplace=False) #This should be replace with imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no nulls now: \n",
      "respondent_id                  0\n",
      "h1n1_concern                   0\n",
      "h1n1_knowledge                 0\n",
      "behavioral_antiviral_meds      0\n",
      "behavioral_avoidance           0\n",
      "behavioral_face_mask           0\n",
      "behavioral_wash_hands          0\n",
      "behavioral_large_gatherings    0\n",
      "behavioral_outside_home        0\n",
      "behavioral_touch_face          0\n",
      "doctor_recc_h1n1               0\n",
      "doctor_recc_seasonal           0\n",
      "chronic_med_condition          0\n",
      "child_under_6_months           0\n",
      "health_worker                  0\n",
      "health_insurance               0\n",
      "opinion_h1n1_vacc_effective    0\n",
      "opinion_h1n1_risk              0\n",
      "opinion_h1n1_sick_from_vacc    0\n",
      "opinion_seas_vacc_effective    0\n",
      "opinion_seas_risk              0\n",
      "opinion_seas_sick_from_vacc    0\n",
      "age_group                      0\n",
      "education                      0\n",
      "race                           0\n",
      "sex                            0\n",
      "income_poverty                 0\n",
      "marital_status                 0\n",
      "rent_or_own                    0\n",
      "employment_status              0\n",
      "hhs_geo_region                 0\n",
      "census_msa                     0\n",
      "household_adults               0\n",
      "household_children             0\n",
      "employment_industry            0\n",
      "employment_occupation          0\n",
      "h1n1_vaccine                   0\n",
      "seasonal_vaccine               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "handling_nulls = \"dropall\"\n",
    "if handling_nulls == \"iterative\":\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train.replace(-1, np.nan) \n",
    "    df_train.replace(\"no_response\", np.nan) \n",
    "    \n",
    "    # SMOTE Sampling\n",
    "    temp_columns = df_train.columns\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    imp.fit(df_train)\n",
    "    df_train = pd.DataFrame(data=imp.transform(df_train))\n",
    "    df_train.columns = temp_columns\n",
    "    df_train\n",
    "elif handling_nulls == \"median\":\n",
    "    df_train_median = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_median.replace(-1, np.nan) \n",
    "    df_train_median.replace(\"no_response\", np.nan) \n",
    "    df_train_median.fillna(df_train.median())\n",
    "elif handling_nulls == \"dropall\":\n",
    "    df_train_drop = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_drop.replace(-1, np.nan) \n",
    "    df_train_drop.replace(\"no_response\", np.nan) \n",
    "    # See how it is if we drop the NaNs\n",
    "    df_train_drop = df_train.dropna(inplace=False) #This should be replace with imputation.\n",
    "p(\"There should be no nulls now: \")\n",
    "p(df_train_drop.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respondent_id are all unique so its irrelevant now that we merged.\n",
    "df_train = df_train.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_label = df_train_label.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_median = df_train_median.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_drop = df_train_drop.drop(columns=['respondent_id'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "\n",
    "# Separating the features and targets\n",
    "# Original Data\n",
    "X_h1n1 = df_train.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_seasonal = X_h1n1\n",
    "y_h1n1 = df_train['h1n1_vaccine']\n",
    "y_seasonal = df_train['seasonal_vaccine']\n",
    "X_h1n1, y_h1n1 = oversample.fit_resample(X_h1n1, y_h1n1)\n",
    "X_seasonal, y_seasonal = oversample.fit_resample(X_seasonal, y_seasonal)\n",
    "\n",
    "# Encoded Data\n",
    "X_label_h1n1 = df_train_label.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_label_seasonal = X_label_h1n1\n",
    "y_label_h1n1 = df_train_label['h1n1_vaccine']\n",
    "y_label_seasonal = df_train_label['seasonal_vaccine']\n",
    "X_label_h1n1, y_label_h1n1 = oversample.fit_resample(X_label_h1n1, y_label_h1n1)\n",
    "X_label_seasonal, y_label_seasonal = oversample.fit_resample(X_label_seasonal, y_label_seasonal)\n",
    "\n",
    "# Nulls replaced with median data\n",
    "X_median_h1n1 = df_train_median.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_median_seasonal = X_median_h1n1\n",
    "y_median_h1n1 = df_train_median['h1n1_vaccine']\n",
    "y_median_seasonal = df_train_median['seasonal_vaccine']\n",
    "X_median_h1n1, y_median_h1n1 = oversample.fit_resample(X_median_h1n1, y_median_h1n1)\n",
    "X_median_seasonal, y_median_seasonal = oversample.fit_resample(X_median_seasonal, y_median_seasonal)\n",
    "\n",
    "# Nulls dropped data\n",
    "X_drop_h1n1 = df_train_drop.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_drop_seasonal = X_drop_h1n1\n",
    "y_drop_h1n1 = df_train_drop['h1n1_vaccine']\n",
    "y_drop_seasonal = df_train_drop['seasonal_vaccine']\n",
    "X_drop_h1n1, y_drop_h1n1 = oversample.fit_resample(X_drop_h1n1, y_drop_h1n1)\n",
    "X_drop_seasonal, y_drop_seasonal = oversample.fit_resample(X_drop_seasonal, y_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Train-Test-Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into 70-20-10 train-test-validation sets\n",
    "\n",
    "# Original Data\n",
    "X_train_h1n1, X_test_h1n1, y_train_h1n1, y_test_h1n1 = train_test_split(X_h1n1, y_h1n1, train_size=.7)\n",
    "X_test_h1n1, X_val_h1n1, y_test_h1n1, y_val_h1n1 = train_test_split(X_test_h1n1, y_test_h1n1, train_size=.67)\n",
    "\n",
    "X_train_seasonal, X_test_seasonal, y_train_seasonal, y_test_seasonal = train_test_split(X_seasonal, y_seasonal, train_size=.7)\n",
    "X_test_seasonal, X_val_seasonal, y_test_seasonal, y_val_seasonal = train_test_split(X_test_seasonal, y_test_seasonal, train_size=.67)\n",
    "\n",
    "# Encoded Data\n",
    "X_train_label_h1n1, X_test_label_h1n1, y_train_label_h1n1, y_test_label_h1n1 = train_test_split(X_label_h1n1, y_label_h1n1, train_size=.7)\n",
    "X_test_label_h1n1, X_val_label_h1n1, y_test_label_h1n1, y_val_label_h1n1 = train_test_split(X_test_label_h1n1, y_test_label_h1n1, train_size=.67)\n",
    "\n",
    "X_train_label_seasonal, X_test_label_seasonal, y_train_label_seasonal, y_test_label_seasonal = train_test_split(X_label_seasonal, y_label_seasonal, train_size=.7)\n",
    "X_test_label_seasonal, X_val_label_seasonal, y_test_label_seasonal, y_val_label_seasonal = train_test_split(X_test_label_seasonal, y_test_label_seasonal, train_size=.67)\n",
    "\n",
    "# Nulls replaced with median data\n",
    "X_train_median_h1n1, X_test_median_h1n1, y_train_median_h1n1, y_test_median_h1n1 = train_test_split(X_median_h1n1, y_median_h1n1, train_size=.7)\n",
    "X_test_median_h1n1, X_val_median_h1n1, y_test_median_h1n1, y_val_median_h1n1 = train_test_split(X_test_median_h1n1, y_test_median_h1n1, train_size=.67)\n",
    "\n",
    "X_train_median_seasonal, X_test_median_seasonal, y_train_median_seasonal, y_test_median_seasonal = train_test_split(X_median_seasonal, y_median_seasonal, train_size=.7)\n",
    "X_test_median_seasonal, X_val_median_seasonal, y_test_median_seasonal, y_val_median_seasonal = train_test_split(X_test_median_seasonal, y_test_median_seasonal, train_size=.67)\n",
    "\n",
    "# Nulls dropped data\n",
    "X_train_drop_h1n1, X_test_drop_h1n1, y_train_drop_h1n1, y_test_drop_h1n1 = train_test_split(X_drop_h1n1, y_drop_h1n1, train_size=.7)\n",
    "X_test_drop_h1n1, X_val_drop_h1n1, y_test_drop_h1n1, y_val_drop_h1n1 = train_test_split(X_test_drop_h1n1, y_test_drop_h1n1, train_size=.67)\n",
    "\n",
    "X_train_drop_seasonal, X_test_drop_seasonal, y_train_drop_seasonal, y_test_drop_seasonal = train_test_split(X_drop_seasonal, y_drop_seasonal, train_size=.7)\n",
    "X_test_drop_seasonal, X_val_drop_seasonal, y_test_drop_seasonal, y_val_drop_seasonal = train_test_split(X_test_drop_seasonal, y_test_drop_seasonal, train_size=.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Models to Determine Best Handling of Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network (using scaler inputs) f1 score:  0.334\n",
      "Confusion Matrix (Accuracy 0.5015)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 4240    0\n",
      "     1 4215    0\n",
      "\n",
      "Neural Network (using scaler inputs) f1 score:  0.332\n",
      "Confusion Matrix (Accuracy 0.4964)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2848    0\n",
      "     1 2889    0\n",
      "\n",
      "Neural Network - Encoded H1N1 (using scaler inputs) f1 score:  0.334\n",
      "Confusion Matrix (Accuracy 0.5022)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 4246    0\n",
      "     1 4209    0\n",
      "\n",
      "Neural Network - Encoded Seasonal Flu (using scaler inputs) f1 score:  0.332\n",
      "Confusion Matrix (Accuracy 0.4964)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0    0 2889\n",
      "     1    0 2848\n",
      "\n",
      "Neural Network - Median Data H1N1 (using scaler inputs) f1 score:  0.332\n",
      "Confusion Matrix (Accuracy 0.4973)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 4205    0\n",
      "     1 4250    0\n",
      "\n",
      "Neural Network - Median Data Seasonal Flu (using scaler inputs) f1 score:  0.331\n",
      "Confusion Matrix (Accuracy 0.4942)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0    0 2902\n",
      "     1    0 2835\n",
      "\n",
      "Neural Network - Drop Data H1N1 (using scaler inputs) f1 score:  0.332\n",
      "Confusion Matrix (Accuracy 0.4978)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0    0 4246\n",
      "     1    0 4209\n",
      "\n",
      "Neural Network - Drop Data Seasonal Flu (using scaler inputs) f1 score:  0.33\n",
      "Confusion Matrix (Accuracy 0.4919)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2822    0\n",
      "     1 2915    0\n"
     ]
    }
   ],
   "source": [
    "# Neural Network Model\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), max_iter = 1000, random_state = 12345)\n",
    "\n",
    "# Original Data\n",
    "NN.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = NN.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nNeural Network (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "NN.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = NN.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nNeural Network (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "NN.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = NN.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Encoded H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "NN.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = NN.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Encoded Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "NN.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = NN.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Median Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "NN.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = NN.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Median Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "NN.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = NN.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Drop Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "NN.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = NN.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nNeural Network - Drop Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models predict one of the classes for all predictions, leading to minimal differences in dataset performance. The original and encoded data do very slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression H1N1 f1 score:  0.795\n",
      "Confusion Matrix (Accuracy 0.7948)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3386  854\n",
      "     1  881 3334\n",
      "\n",
      "Logistic Regression Seasonal Flu f1 score:  0.762\n",
      "Confusion Matrix (Accuracy 0.7622)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2184  664\n",
      "     1  700 2189\n",
      "\n",
      "Logistic Regression - Encoded H1N1 f1 score:  0.797\n",
      "Confusion Matrix (Accuracy 0.7966)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3360  886\n",
      "     1  834 3375\n",
      "\n",
      "Logistic Regression - Encoded Seasonal Flu f1 score:  0.763\n",
      "Confusion Matrix (Accuracy 0.7629)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2209  680\n",
      "     1  680 2168\n",
      "\n",
      "Logistic Regression - Median Data H1N1 f1 score:  0.794\n",
      "Confusion Matrix (Accuracy 0.7942)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3380  825\n",
      "     1  915 3335\n",
      "\n",
      "Logistic Regression - Median Data Seasonal Flu f1 score:  0.766\n",
      "Confusion Matrix (Accuracy 0.7661)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2227  675\n",
      "     1  667 2168\n",
      "\n",
      "Logistic Regression - Drop Data H1N1 f1 score:  0.802\n",
      "Confusion Matrix (Accuracy 0.8025)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3425  821\n",
      "     1  849 3360\n",
      "\n",
      "Logistic Regression - Drop Data Seasonal Flu f1 score:  0.771\n",
      "Confusion Matrix (Accuracy 0.7715)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2216  606\n",
      "     1  705 2210\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logistic = LogisticRegressionCV(cv=5, penalty = 'l2', solver = 'liblinear',tol=1e-5,max_iter=1000,Cs=10, random_state = 12345)\n",
    "\n",
    "# Original Data\n",
    "logistic.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = logistic.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "logistic.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = logistic.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "logistic.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = logistic.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Encoded H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "logistic.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = logistic.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Encoded Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "logistic.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = logistic.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Median Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "logistic.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = logistic.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Median Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "logistic.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = logistic.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Drop Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "logistic.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = logistic.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nLogistic Regression - Drop Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the data seems best in the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM H1N1 f1 score:  0.799\n",
      "Confusion Matrix (Accuracy 0.7994)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3427  813\n",
      "     1  883 3332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Seasonal f1 score:  0.765\n",
      "Confusion Matrix (Accuracy 0.7654)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2244  604\n",
      "     1  742 2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Encoded H1N1 f1 score:  0.796\n",
      "Confusion Matrix (Accuracy 0.7960)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3346  900\n",
      "     1  825 3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Encoded Seasonal Flu f1 score:  0.767\n",
      "Confusion Matrix (Accuracy 0.7670)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2238  651\n",
      "     1  686 2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Median Data H1N1 f1 score:  0.802\n",
      "Confusion Matrix (Accuracy 0.8019)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3455  750\n",
      "     1  925 3325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Median Data Seasonal Flu f1 score:  0.768\n",
      "Confusion Matrix (Accuracy 0.7685)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2291  611\n",
      "     1  717 2118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM - Drop Data H1N1 f1 score:  0.808\n",
      "Confusion Matrix (Accuracy 0.8078)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3465  781\n",
      "     1  844 3365\n",
      "\n",
      "SVM - Drop Data Seasonal Flu f1 score:  0.776\n",
      "Confusion Matrix (Accuracy 0.7760)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2261  561\n",
      "     1  724 2191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Support Vector Machine Model\n",
    "SVM = svm.LinearSVC(max_iter = 5000, penalty = 'l2', loss = 'hinge', random_state = 12345)\n",
    "\n",
    "# Original Data\n",
    "SVM.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = SVM.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nSVM H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "SVM.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = SVM.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nSVM Seasonal f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "SVM.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = SVM.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nSVM - Encoded H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "SVM.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = SVM.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nSVM - Encoded Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "SVM.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = SVM.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nSVM - Median Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "SVM.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = SVM.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nSVM - Median Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "SVM.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = SVM.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nSVM - Drop Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "SVM.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = SVM.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nSVM - Drop Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dropped datasets also works has the best scores for the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GradientBoosting (using scaler inputs) f1 score:  0.903\n",
      "Confusion Matrix (Accuracy 0.9036)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3969  271\n",
      "     1  544 3671\n",
      "\n",
      "GradientBoosting (using scaler inputs) f1 score:  0.795\n",
      "Confusion Matrix (Accuracy 0.7947)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2270  578\n",
      "     1  600 2289\n",
      "\n",
      "GradientBoosting - Encoded H1N1 (using scaler inputs) f1 score:  0.907\n",
      "Confusion Matrix (Accuracy 0.9069)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3985  261\n",
      "     1  526 3683\n",
      "\n",
      "GradientBoosting - Encoded Seasonal Flu (using scaler inputs) f1 score:  0.798\n",
      "Confusion Matrix (Accuracy 0.7976)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2324  565\n",
      "     1  596 2252\n",
      "\n",
      "GradientBoosting - Median Data H1N1 (using scaler inputs) f1 score:  0.903\n",
      "Confusion Matrix (Accuracy 0.9036)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3961  244\n",
      "     1  571 3679\n",
      "\n",
      "GradientBoosting - Median Data Seasonal Flu (using scaler inputs) f1 score:  0.788\n",
      "Confusion Matrix (Accuracy 0.7877)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2324  578\n",
      "     1  640 2195\n",
      "\n",
      "GradientBoosting - Drop Data H1N1 (using scaler inputs) f1 score:  0.906\n",
      "Confusion Matrix (Accuracy 0.9059)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 4024  222\n",
      "     1  574 3635\n",
      "\n",
      "GradientBoosting - Drop Data Seasonal Flu (using scaler inputs) f1 score:  0.805\n",
      "Confusion Matrix (Accuracy 0.8055)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2306  516\n",
      "     1  600 2315\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_classif = GradientBoostingClassifier()\n",
    "\n",
    "# Original Data\n",
    "gb_classif.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = gb_classif.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "gb_classif.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = gb_classif.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "gb_classif.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = gb_classif.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Encoded H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "gb_classif.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = gb_classif.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Encoded Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "gb_classif.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = gb_classif.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Median Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "gb_classif.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = gb_classif.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Median Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "gb_classif.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = gb_classif.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Drop Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "gb_classif.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = gb_classif.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nGradientBoosting - Drop Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoded data has best metrics for the H1N1 model, slightly lower than the dropped data model, while the seasonal flu model does best with the dropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes f1 score:  0.747\n",
      "Confusion Matrix (Accuracy 0.7473)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2969 1271\n",
      "     1  866 3349\n",
      "\n",
      "Naive Bayes f1 score:  0.726\n",
      "Confusion Matrix (Accuracy 0.7260)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1981  867\n",
      "     1  705 2184\n",
      "\n",
      "Naive Bayes - Encoded H1N1 (using scaler inputs) f1 score:  0.745\n",
      "Confusion Matrix (Accuracy 0.7455)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2958 1288\n",
      "     1  864 3345\n",
      "\n",
      "Naive Bayes - Encoded Seasonal Flu (using scaler inputs) f1 score:  0.732\n",
      "Confusion Matrix (Accuracy 0.7324)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2023  866\n",
      "     1  669 2179\n",
      "\n",
      "Naive Bayes - Median Data H1N1 (using scaler inputs) f1 score:  0.746\n",
      "Confusion Matrix (Accuracy 0.7459)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2980 1225\n",
      "     1  923 3327\n",
      "\n",
      "Naive Bayes - Median Data Seasonal Flu (using scaler inputs) f1 score:  0.725\n",
      "Confusion Matrix (Accuracy 0.7248)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2018  884\n",
      "     1  695 2140\n",
      "\n",
      "Naive Bayes - Drop Data H1N1 (using scaler inputs) f1 score:  0.761\n",
      "Confusion Matrix (Accuracy 0.7612)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3070 1176\n",
      "     1  843 3366\n",
      "\n",
      "Naive Bayes - Drop Data Seasonal Flu (using scaler inputs) f1 score:  0.73\n",
      "Confusion Matrix (Accuracy 0.7309)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1981  841\n",
      "     1  703 2212\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Original Data\n",
    "gnb.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = gnb.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "gnb.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = gnb.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "gnb.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = gnb.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Encoded H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "gnb.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = gnb.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Encoded Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "gnb.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = gnb.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Median Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "gnb.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = gnb.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Median Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "gnb.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = gnb.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Drop Data H1N1 (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "gnb.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = gnb.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nNaive Bayes - Drop Data Seasonal Flu (using scaler inputs) f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasonal flu model does best with the encoded data while the Naive Bayes model does best with the Dropped Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN f1 score:  0.816\n",
      "Confusion Matrix (Accuracy 0.8202)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2813 1427\n",
      "     1   93 4122\n",
      "\n",
      "KNN f1 score:  0.723\n",
      "Confusion Matrix (Accuracy 0.7230)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1962  886\n",
      "     1  703 2186\n",
      "\n",
      "KNN - Encoded H1N1 f1 score:  0.817\n",
      "Confusion Matrix (Accuracy 0.8216)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2814 1432\n",
      "     1   76 4133\n",
      "\n",
      "KNN - Encoded Seasonal Flu f1 score:  0.714\n",
      "Confusion Matrix (Accuracy 0.7147)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1917  972\n",
      "     1  665 2183\n",
      "\n",
      "KNN - Median Data H1N1 f1 score:  0.805\n",
      "Confusion Matrix (Accuracy 0.8111)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2693 1512\n",
      "     1   85 4165\n",
      "\n",
      "KNN - Median Data Seasonal Flu f1 score:  0.718\n",
      "Confusion Matrix (Accuracy 0.7185)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1961  941\n",
      "     1  674 2161\n",
      "\n",
      "KNN - Drop Data H1N1 f1 score:  0.818\n",
      "Confusion Matrix (Accuracy 0.8216)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2837 1409\n",
      "     1   99 4110\n",
      "\n",
      "KNN - Drop Data Seasonal Flu f1 score:  0.72\n",
      "Confusion Matrix (Accuracy 0.7208)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 1943  879\n",
      "     1  723 2192\n"
     ]
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3, weights = 'distance')\n",
    "\n",
    "# Original Data\n",
    "knn.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = knn.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nKNN f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "knn.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = knn.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nKNN f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "knn.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = knn.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nKNN - Encoded H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "knn.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = knn.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nKNN - Encoded Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "knn.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = knn.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nKNN - Median Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "knn.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = knn.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nKNN - Median Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "knn.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = knn.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nKNN - Drop Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "knn.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = knn.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nKNN - Drop Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H1N1 model does best with the dropped data, while the seasonal flu model does best with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest f1 score:  0.825\n",
      "Confusion Matrix (Accuracy 0.8251)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3613  627\n",
      "     1  852 3363\n",
      "\n",
      "Random Forest f1 score:  0.75\n",
      "Confusion Matrix (Accuracy 0.7506)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2237  611\n",
      "     1  820 2069\n",
      "\n",
      "Random Forest - Encoded H1N1 f1 score:  0.828\n",
      "Confusion Matrix (Accuracy 0.8283)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3605  641\n",
      "     1  811 3398\n",
      "\n",
      "Random Forest - Encoded Seasonal Flu f1 score:  0.759\n",
      "Confusion Matrix (Accuracy 0.7591)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2280  609\n",
      "     1  773 2075\n",
      "\n",
      "Random Forest - Median Data H1N1 f1 score:  0.823\n",
      "Confusion Matrix (Accuracy 0.8232)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3594  611\n",
      "     1  884 3366\n",
      "\n",
      "Random Forest - Median Data Seasonal Flu f1 score:  0.744\n",
      "Confusion Matrix (Accuracy 0.7439)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2253  649\n",
      "     1  820 2015\n",
      "\n",
      "Random Forest - Drop Data H1N1 f1 score:  0.823\n",
      "Confusion Matrix (Accuracy 0.8231)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3633  613\n",
      "     1  883 3326\n",
      "\n",
      "Random Forest - Drop Data Seasonal Flu f1 score:  0.766\n",
      "Confusion Matrix (Accuracy 0.7659)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2303  519\n",
      "     1  824 2091\n"
     ]
    }
   ],
   "source": [
    "# Random Forest model\n",
    "rf = RandomForestClassifier(max_depth=2, random_state = 12345)\n",
    "\n",
    "# Original Data\n",
    "rf.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = rf.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nRandom Forest f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "rf.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = rf.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nRandom Forest f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "rf.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = rf.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Encoded H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "rf.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = rf.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Encoded Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "rf.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = rf.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Median Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "rf.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = rf.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Median Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "rf.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = rf.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Drop Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "rf.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = rf.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nRandom Forest - Drop Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H1N1 model does best with the encoded data, while the Seasonal Flu model does best with the dropped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adaboost f1 score:  0.898\n",
      "Confusion Matrix (Accuracy 0.8980)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3896  344\n",
      "     1  518 3697\n",
      "\n",
      "Adaboost f1 score:  0.791\n",
      "Confusion Matrix (Accuracy 0.7910)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2267  581\n",
      "     1  618 2271\n",
      "\n",
      "Adaboost - Encoded H1N1 f1 score:  0.897\n",
      "Confusion Matrix (Accuracy 0.8969)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3891  355\n",
      "     1  517 3692\n",
      "\n",
      "Adaboost - Encoded Seasonal Flu f1 score:  0.783\n",
      "Confusion Matrix (Accuracy 0.7828)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2293  596\n",
      "     1  650 2198\n",
      "\n",
      "Adaboost - Median Data H1N1 f1 score:  0.898\n",
      "Confusion Matrix (Accuracy 0.8976)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3883  322\n",
      "     1  544 3706\n",
      "\n",
      "Adaboost - Median Data Seasonal Flu f1 score:  0.768\n",
      "Confusion Matrix (Accuracy 0.7680)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2243  659\n",
      "     1  672 2163\n",
      "\n",
      "Adaboost - Drop Data H1N1 f1 score:  0.897\n",
      "Confusion Matrix (Accuracy 0.8969)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 3926  320\n",
      "     1  552 3657\n",
      "\n",
      "Adaboost - Drop Data Seasonal Flu f1 score:  0.777\n",
      "Confusion Matrix (Accuracy 0.7769)\n",
      "\n",
      "       Prediction\n",
      "Actual    0    1\n",
      "     0 2217  605\n",
      "     1  675 2240\n"
     ]
    }
   ],
   "source": [
    "# Boosted Classifier\n",
    "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), learning_rate = 1.5, n_estimators=400, random_state = 12345)\n",
    "\n",
    "# Original Data\n",
    "adaboost.fit(X_train_h1n1, y_train_h1n1)\n",
    "y_pred_h1n1 = adaboost.predict(X_test_h1n1)\n",
    "f1 = round(f1_score(y_test_h1n1, y_pred_h1n1, average='macro'), 3)\n",
    "print(\"\\nAdaboost f1 score: \", f1)\n",
    "classificationSummary(y_test_h1n1, y_pred_h1n1)\n",
    "\n",
    "adaboost.fit(X_train_seasonal, y_train_seasonal)\n",
    "y_pred_seasonal = adaboost.predict(X_test_seasonal)\n",
    "f1 = round(f1_score(y_test_seasonal, y_pred_seasonal, average='macro'), 3)\n",
    "print(\"\\nAdaboost f1 score: \", f1)\n",
    "classificationSummary(y_test_seasonal, y_pred_seasonal)\n",
    "\n",
    "#Encoded Data\n",
    "adaboost.fit(X_train_label_h1n1, y_train_label_h1n1)\n",
    "y_pred_label_h1n1 = adaboost.predict(X_test_label_h1n1)\n",
    "f1 = round(f1_score(y_test_label_h1n1, y_pred_label_h1n1, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Encoded H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_label_h1n1, y_pred_label_h1n1)\n",
    "\n",
    "adaboost.fit(X_train_label_seasonal, y_train_label_seasonal)\n",
    "y_pred_label_seasonal = adaboost.predict(X_test_label_seasonal)\n",
    "f1 = round(f1_score(y_test_label_seasonal, y_pred_label_seasonal, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Encoded Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_label_seasonal, y_pred_label_seasonal)\n",
    "\n",
    "# Median Data\n",
    "adaboost.fit(X_train_median_h1n1, y_train_median_h1n1)\n",
    "y_pred_median_h1n1 = adaboost.predict(X_test_median_h1n1)\n",
    "f1 = round(f1_score(y_test_median_h1n1, y_pred_median_h1n1, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Median Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_median_h1n1, y_pred_median_h1n1)\n",
    "\n",
    "adaboost.fit(X_train_median_seasonal, y_train_median_seasonal)\n",
    "y_pred_median_seasonal = adaboost.predict(X_test_median_seasonal)\n",
    "f1 = round(f1_score(y_test_median_seasonal, y_pred_median_seasonal, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Median Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_median_seasonal, y_pred_median_seasonal)\n",
    "\n",
    "# Nulls Dropped Data\n",
    "adaboost.fit(X_train_drop_h1n1, y_train_drop_h1n1)\n",
    "y_pred_drop_h1n1 = adaboost.predict(X_test_drop_h1n1)\n",
    "f1 = round(f1_score(y_test_drop_h1n1, y_pred_drop_h1n1, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Drop Data H1N1 f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_h1n1, y_pred_drop_h1n1)\n",
    "\n",
    "adaboost.fit(X_train_drop_seasonal, y_train_drop_seasonal)\n",
    "y_pred_drop_seasonal = adaboost.predict(X_test_drop_seasonal)\n",
    "f1 = round(f1_score(y_test_drop_seasonal, y_pred_drop_seasonal, average='macro'), 3)\n",
    "print(\"\\nAdaboost - Drop Data Seasonal Flu f1 score: \", f1)\n",
    "classificationSummary(y_test_drop_seasonal, y_pred_drop_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The H1N1 and Seasonal Flu models do best with the original data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization with sklearn\n",
    "\n",
    "# Fitting a scaler on the training datasets\n",
    "normh1n1= MinMaxScaler().fit(X_train_h1n1)\n",
    "normseasonal = MinMaxScaler().fit(X_train_seasonal)\n",
    "\n",
    "# Transforming the training datasets\n",
    "X_train_norm_h1n1 = normh1n1.transform(X_train_h1n1)\n",
    "X_train_norm_seasonal = normseasonal.transform(X_train_seasonal)\n",
    "\n",
    "# transform the testing dataset\n",
    "X_test_norm_h1n1 = normh1n1.transform(X_test_h1n1)\n",
    "X_test_norm_seasonal = normseasonal.transform(X_test_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization with sklearn\n",
    "\n",
    "# Copy the two datasets\n",
    "X_train_stand_h1n1 = X_train_h1n1.copy()\n",
    "X_train_stand_seasonal = X_train_seasonal.copy()\n",
    "X_test_stand_h1n1 = X_test_h1n1.copy()\n",
    "X_test_stand_seasonal = X_test_seasonal.copy()\n",
    "\n",
    "# Group the numerical features and not categorical\n",
    "\n",
    "num_cols = ['h1n1_concern','h1n1_knowledge','behavioral_antiviral_meds','behavioral_avoidance','behavioral_face_mask','behavioral_large_gatherings',\n",
    "'behavioral_outside_home','behavioral_wash_hands','behavioral_touch_face','doctor_recc_h1n1','doctor_recc_seasonal','chronic_med_condition',\n",
    "'child_under_6_months','health_worker','health_insurance','opinion_h1n1_vacc_effective','opinion_h1n1_sick_from_vacc','opinion_h1n1_risk',\n",
    "'opinion_seas_sick_from_vacc','household_adults','household_children']\n",
    "\n",
    "# Apply standardization on the numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # Fit the scaler on the training data column\n",
    "    scale_h1n1 = StandardScaler().fit(X_train_stand_h1n1[[i]])\n",
    "    scale_seasonal = StandardScaler().fit(X_train_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the training data column\n",
    "    X_train_stand_h1n1[i] = scale_h1n1.transform(X_train_stand_h1n1[[i]])\n",
    "    X_train_stand_seasonal[i] = scale_seasonal.transform(X_train_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the testing data column\n",
    "    X_test_stand_h1n1[i] = scale_h1n1.transform(X_test_stand_h1n1[[i]])\n",
    "    X_test_stand_seasonal[i] = scale_seasonal.transform(X_test_stand_seasonal[[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Normalized/Standardized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.501478</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.496427</td>\n",
       "      <td>0.332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.812300</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.773575</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.811591</td>\n",
       "      <td>0.812</td>\n",
       "      <td>0.772878</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.501478    0.334  0.496427   0.332\n",
       "Normalized    0.812300    0.812  0.773575   0.774\n",
       "Standardized  0.811591    0.812  0.772878   0.773"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training a Neural Network model\n",
    "\n",
    "# Neural Network Model\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), max_iter = 1000, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = NN.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = NN.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_svr = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_svr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized data performs best for the Neural Network Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.794796</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.762245</td>\n",
       "      <td>0.762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.794796</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.762594</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.794914</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.762594</td>\n",
       "      <td>0.763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.794796    0.795  0.762245   0.762\n",
       "Normalized    0.794796    0.795  0.762594   0.763\n",
       "Standardized  0.794914    0.795  0.762594   0.763"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logistic = LogisticRegressionCV(cv=5, penalty = 'l2', solver = 'liblinear',tol=1e-5,max_iter=1000,Cs=10, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = logistic.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = logistic.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_logistic = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the standardized H1N1 model ahs higher accuracy than the others, but otherwise the normalized and standardized data have similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.799409</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.765383</td>\n",
       "      <td>0.765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.800473</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.765731</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.800473</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.766080</td>\n",
       "      <td>0.766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.799409    0.799  0.765383   0.765\n",
       "Normalized    0.800473    0.800  0.765731   0.766\n",
       "Standardized  0.800473    0.800  0.766080   0.766"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine Model\n",
    "SVM = svm.LinearSVC(max_iter = 5000, penalty = 'l2', loss = 'hinge', random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = SVM.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = SVM.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_SVM = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standardized data has slightly better accuracy for the flu model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.794666</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.794666</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.903607</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.794666</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.903607    0.903  0.794666   0.795\n",
       "Normalized    0.903607    0.903  0.794666   0.795\n",
       "Standardized  0.903607    0.903  0.794666   0.795"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_classif = GradientBoostingClassifier()\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = gb_classif.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = gb_classif.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_gb_classif = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_gb_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting Classifier models all have similar metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.725989</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.725989</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.74725</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.725989</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original       0.74725    0.747  0.725989   0.726\n",
       "Normalized     0.74725    0.747  0.725989   0.726\n",
       "Standardized   0.74725    0.747  0.725989   0.726"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gaussian Naive Bayes model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = gnb.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = gnb.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_gnb = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_gnb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes models don't have a statistical difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.729301</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.824128</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.720934</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.815494</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.736273</td>\n",
       "      <td>0.736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.808989    0.804  0.729301   0.729\n",
       "Normalized    0.824128    0.821  0.720934   0.721\n",
       "Standardized  0.815494    0.811  0.736273   0.736"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights = 'distance')\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = knn.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = knn.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_knn = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalized data has higher metrics for the H1N1 models, but the standardized data has higher metrics for the seasonal flu models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.825074</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.750566</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.825074</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.750566</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.825074</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.750566</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.825074    0.825  0.750566    0.75\n",
       "Normalized    0.825074    0.825  0.750566    0.75\n",
       "Standardized  0.825074    0.825  0.750566    0.75"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest model\n",
    "rf = RandomForestClassifier(max_depth=2, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = rf.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = rf.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_rf = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest models don't have a statistical difference between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.898048</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.791006</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Normalized</th>\n",
       "      <td>0.898048</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.791006</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standardized</th>\n",
       "      <td>0.897694</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.791006</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original      0.898048    0.898  0.791006   0.791\n",
       "Normalized    0.898048    0.898  0.791006   0.791\n",
       "Standardized  0.897694    0.898  0.791006   0.791"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosted Classifier\n",
    "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), learning_rate = 1.5, n_estimators=400, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Normalized and standardized training and testing data\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_norm_h1n1, X_train_stand_h1n1]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_norm_h1n1, X_test_stand_h1n1]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_norm_seasonal, X_train_stand_seasonal]\n",
    "testX_seasonal = [X_test_seasonal, X_test_norm_seasonal, X_test_stand_seasonal]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # model fitting\n",
    "    model = adaboost.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    # measuring RMSE\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # model fitting\n",
    "    model = adaboost.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # measuring RMSE\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# results    \n",
    "df_adaboost = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Normalized','Standardized'])\n",
    "df_adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boosted Classifier models have similar metrics, but the standardized H1N1 model has lower accuracy than the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining Models\n",
    "\n",
    "### Combining optimal handling of null values, normalization/standardization, and running them on full datasets or only highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting highly correlated variables\n",
    "flu_correlated = ['h1n1_concern', 'h1n1_knowledge', 'behavior_wash_hands', 'behavior_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment occupation']\n",
    "h1n1_correlated = ['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the dropped H1N1 data\n",
    "X_train_drop_h1n1_corr = X_train_drop_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_test_drop_h1n1_corr = X_test_drop_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "\n",
    "# Fitting a scaler on the training datasets\n",
    "normh1n1_drop = MinMaxScaler().fit(X_train_drop_h1n1)\n",
    "normh1n1_drop_corr = MinMaxScaler().fit(X_train_drop_h1n1_corr)\n",
    "\n",
    "# Transforming the training datasets\n",
    "X_train_drop_norm_h1n1 = normh1n1_drop.transform(X_train_drop_h1n1)\n",
    "X_train_drop_norm_h1n1_corr = normh1n1_drop_corr.transform(X_train_drop_h1n1_corr)\n",
    "\n",
    "\n",
    "# transform the testing dataset\n",
    "X_test_drop_norm_h1n1 = normh1n1_drop.transform(X_test_drop_h1n1)\n",
    "X_test_drop_norm_h1n1_corr = normh1n1_drop_corr.transform(X_test_drop_h1n1_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the dropped H1N1 and Seasonal Data\n",
    "\n",
    "# Copy the two datasets\n",
    "X_train_drop_stand_h1n1 = X_train_drop_h1n1.copy()\n",
    "X_train_drop_stand_seasonal = X_train_drop_seasonal.copy()\n",
    "X_test_drop_stand_h1n1 = X_test_drop_h1n1.copy()\n",
    "X_test_drop_stand_seasonal = X_test_drop_seasonal.copy()\n",
    "\n",
    "# Apply standardization on the numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # Fit the scaler on the training data column\n",
    "    scale_drop_h1n1 = StandardScaler().fit(X_train_drop_stand_h1n1[[i]])\n",
    "    scale_drop_seasonal = StandardScaler().fit(X_train_drop_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the training data column\n",
    "    X_train_drop_stand_h1n1[i] = scale_drop_h1n1.transform(X_train_drop_stand_h1n1[[i]])\n",
    "    X_train_drop_stand_seasonal[i] = scale_drop_seasonal.transform(X_train_drop_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the testing data column\n",
    "    X_test_drop_stand_h1n1[i] = scale_drop_h1n1.transform(X_test_drop_stand_h1n1[[i]])\n",
    "    X_test_drop_stand_seasonal[i] = scale_drop_seasonal.transform(X_test_drop_stand_seasonal[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating correlated datasets\n",
    "X_train_h1n1_corr = X_train_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_train_seasonal_corr = X_train_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "X_test_h1n1_corr = X_test_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_test_seasonal_corr = X_test_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "\n",
    "X_train_drop_stand_h1n1_corr = X_train_drop_stand_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_train_drop_stand_seasonal_corr = X_train_drop_stand_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "X_test_drop_stand_h1n1_corr = X_test_drop_stand_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_test_drop_stand_seasonal_corr = X_test_drop_stand_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "\n",
    "X_train_label_h1n1_corr = X_train_label_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_train_label_seasonal_corr = X_train_label_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "X_test_label_h1n1_corr = X_test_label_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_test_label_seasonal_corr = X_test_label_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "\n",
    "X_train_drop_h1n1_corr = X_train_drop_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_train_drop_seasonal_corr = X_train_drop_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)\n",
    "X_test_drop_h1n1_corr = X_test_drop_h1n1.filter(['h1n1_concern', 'h1n1_knowledge', 'doctor_recc_h1n1', 'employment_occupation'], axis=1)\n",
    "X_test_drop_seasonal_corr = X_test_drop_seasonal.filter(['h1n1_concern', 'h1n1_knowledge', 'behavioral_wash_hands', 'behavioral_touch_face', 'doctor_recc_h1n1', 'household_children', 'employment_occupation'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.498522</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.496078</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Correlated</th>\n",
       "      <td>0.512005</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.495904</td>\n",
       "      <td>0.440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original           0.498522    0.498  0.496078   0.482\n",
       "Highly Correlated  0.512005    0.512  0.495904   0.440"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Model\n",
    "logistic = LogisticRegressionCV(cv=5, penalty = 'l2', solver = 'liblinear',tol=1e-5,max_iter=1000,Cs=10, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Splitting data into full and correlated datasets\n",
    "trainX_h1n1 = [X_train_drop_stand_h1n1, X_train_drop_stand_h1n1_corr]\n",
    "testX_h1n1 = [X_test_drop_stand_h1n1, X_test_drop_stand_h1n1_corr]\n",
    "\n",
    "trainX_seasonal = [X_train_drop_stand_seasonal, X_train_drop_stand_seasonal_corr]\n",
    "testX_seasonal = [X_test_drop_stand_seasonal, X_test_drop_stand_seasonal_corr]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # Model fitting\n",
    "    model = logistic.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # Model fitting\n",
    "    model = logistic.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # Metrics\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# Results    \n",
    "df_logistic = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Highly Correlated'])\n",
    "df_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highly correlated models do better for H1N1, but not for the Seasonal Flu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LadyBug\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.498167</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.490326</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Correlated</th>\n",
       "      <td>0.504317</td>\n",
       "      <td>0.470</td>\n",
       "      <td>0.493289</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original           0.498167    0.498  0.490326   0.476\n",
       "Highly Correlated  0.504317    0.470  0.493289   0.431"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Support Vector Machine Model\n",
    "SVM = svm.LinearSVC(max_iter = 5000, penalty = 'l2', loss = 'hinge', random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Splitting data into full and correlated datasets\n",
    "trainX_h1n1 = [X_train_drop_norm_h1n1, X_train_drop_norm_h1n1_corr]\n",
    "testX_h1n1 = [X_test_drop_norm_h1n1, X_test_drop_norm_h1n1_corr]\n",
    "\n",
    "trainX_seasonal = [X_train_drop_stand_seasonal, X_train_drop_stand_seasonal_corr]\n",
    "testX_seasonal = [X_test_drop_stand_seasonal, X_test_drop_stand_seasonal_corr]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # Model fitting\n",
    "    model = SVM.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # Model fitting\n",
    "    model = SVM.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # Metrics\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# Results    \n",
    "df_SVM = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Highly Correlated'])\n",
    "df_SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original and highly correlated datasets have mixed metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.497339</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.794666</td>\n",
       "      <td>0.795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Correlated</th>\n",
       "      <td>0.500059</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.665330</td>\n",
       "      <td>0.665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original           0.497339    0.497  0.794666   0.795\n",
       "Highly Correlated  0.500059    0.499  0.665330   0.665"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "gb_classif = GradientBoostingClassifier()\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Splitting data into full and correlated datasets\n",
    "trainX_h1n1 = [X_train_label_h1n1, X_train_label_h1n1_corr]\n",
    "testX_h1n1 = [X_test_label_h1n1, X_test_label_h1n1_corr]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_seasonal_corr]\n",
    "testX_seasonal = [X_test_seasonal, X_test_seasonal_corr]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # Model fitting\n",
    "    model = gb_classif.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # Model fitting\n",
    "    model = gb_classif.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # Metrics\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# Results    \n",
    "df_gb_classif = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Highly Correlated'])\n",
    "df_gb_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the correlated features, the H1N1 has improved metrics while the Seasonal Flu models do better with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.501597</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.499216</td>\n",
       "      <td>0.459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Correlated</th>\n",
       "      <td>0.503134</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.503225</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original           0.501597    0.502  0.499216   0.459\n",
       "Highly Correlated  0.503134    0.498  0.503225   0.469"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest model\n",
    "rf = RandomForestClassifier(max_depth=2, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Splitting data into full and correlated datasets\n",
    "trainX_h1n1 = [X_train_label_h1n1, X_train_label_h1n1_corr]\n",
    "testX_h1n1 = [X_test_label_h1n1, X_test_label_h1n1_corr]\n",
    "\n",
    "trainX_seasonal = [X_train_drop_seasonal, X_train_drop_seasonal_corr]\n",
    "testX_seasonal = [X_test_drop_seasonal, X_test_drop_seasonal_corr]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # Model fitting\n",
    "    model = rf.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # Model fitting\n",
    "    model = rf.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # Metrics\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# Results    \n",
    "df_rf = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Highly Correlated'])\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seasonal flu model does better with the correlated dataset, which is the opposite of most of the models, and the H1N1 doesn't have a significant difference with the highly correlated dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc H1N1</th>\n",
       "      <th>F1 H1N1</th>\n",
       "      <th>Acc Flu</th>\n",
       "      <th>F1 Flu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Original</th>\n",
       "      <td>0.898048</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.791006</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Highly Correlated</th>\n",
       "      <td>0.856180</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.662018</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Acc H1N1  F1 H1N1   Acc Flu  F1 Flu\n",
       "Original           0.898048    0.898  0.791006   0.791\n",
       "Highly Correlated  0.856180    0.856  0.662018   0.662"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boosted Classifier\n",
    "adaboost = AdaBoostClassifier(DecisionTreeClassifier(max_depth = 2), learning_rate = 1.5, n_estimators=400, random_state = 12345)\n",
    "\n",
    "acc_h1n1 = []\n",
    "f1_h1n1 = []\n",
    "acc_seasonal = []\n",
    "f1_seasonal = []\n",
    "\n",
    "# Splitting data into full and correlated datasets\n",
    "trainX_h1n1 = [X_train_h1n1, X_train_h1n1_corr]\n",
    "testX_h1n1 = [X_test_h1n1, X_test_h1n1_corr]\n",
    "\n",
    "trainX_seasonal = [X_train_seasonal, X_train_seasonal_corr]\n",
    "testX_seasonal = [X_test_seasonal, X_test_seasonal_corr]\n",
    "\n",
    "for i in range(len(trainX_h1n1)):   \n",
    "    # Model fitting\n",
    "    model = adaboost.fit(trainX_h1n1[i],y_train_h1n1)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_h1n1[i])\n",
    "    acc_h1n1.append(accuracy_score(y_test_h1n1,pred))\n",
    "    f1_h1n1.append(round(f1_score(y_test_h1n1, pred, average='macro'), 3))\n",
    "    \n",
    "for i in range(len(trainX_seasonal)):   \n",
    "    # Model fitting\n",
    "    model = adaboost.fit(trainX_seasonal[i],y_train_seasonal)    \n",
    "    # Model prediction\n",
    "    pred = model.predict(testX_seasonal[i])\n",
    "    # Metrics\n",
    "    acc_seasonal.append(accuracy_score(y_test_seasonal,pred))\n",
    "    f1_seasonal.append(round(f1_score(y_test_seasonal, pred, average='macro'), 3))\n",
    "    \n",
    "\n",
    "# Results    \n",
    "df_adaboost = pd.DataFrame({'Acc H1N1':acc_h1n1, 'F1 H1N1':f1_h1n1, 'Acc Flu':acc_seasonal, 'F1 Flu':f1_seasonal},index=['Original','Highly Correlated'])\n",
    "df_adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics are better with the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, combining null values and normalization/standardization types lowers the metrics for the models. The top two H1N1 models are the Encoded Gradient Boosted Classifier model (Accuracy: 0.9069) and the Gradient Boosted Classifier model without adjustments (Accuracy: 0.904). The top two Seasonal Flu models are the Dropped NA Gradient Boosted Classifier Model (Accuracy: 0.8055) and the Gradient Boosted Classifier model without adjustments (Accuracy: 0.7951). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best paramters of Adaboost TODO using random search instead of grid search\n",
    "boost_grid = {\n",
    "    'n_estimators': [50, 100, 200, 400],\n",
    "    'learning_rate': [0.01, 0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "}\n",
    "\n",
    "boost_gridSearch = GridSearchCV(AdaBoostClassifier(), boost_grid, cv = 3)\n",
    "boost_gridSearch.fit(X_train, y_train_h1n1)\n",
    "\n",
    "print('Initial Adaboost Parameters:', boost_gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot of the confusion matrix, showing performance of the model\n",
    "cm = confusion_matrix(y_test_h1n1, y_pred)\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     cm.flatten()/np.sum(cm)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
